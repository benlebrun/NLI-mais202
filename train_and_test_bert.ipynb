{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_and_test_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27ef93f71ab54f8fb636f6a6ef38442b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8571c03ed4cb4eeb8fccd3b8d5562ee8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1aa6a32b9d4248e5a073950f7a05582a",
              "IPY_MODEL_94c7b29a235f4231a0e555c32f9a9ea9"
            ]
          }
        },
        "8571c03ed4cb4eeb8fccd3b8d5562ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1aa6a32b9d4248e5a073950f7a05582a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_00a8e56294464f579638016d767f50bd",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e8e155bac072443f950c0a7e41cfdba1"
          }
        },
        "94c7b29a235f4231a0e555c32f9a9ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2b99b8fd0b34c47a5835234477bd44c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 416kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_893f4052f9c74940bfd7d75ea649f123"
          }
        },
        "00a8e56294464f579638016d767f50bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e8e155bac072443f950c0a7e41cfdba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2b99b8fd0b34c47a5835234477bd44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "893f4052f9c74940bfd7d75ea649f123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY8zAHBnB6Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRY1X-PaCElz",
        "colab_type": "text"
      },
      "source": [
        "### Using a GPU for faster training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HondY3KCBBm",
        "colab_type": "code",
        "outputId": "f05866ea-a02a-4ce3-8fc9-e3c34120cf9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# use GPU\n",
        "device = torch.device(\"cuda\")\n",
        "# confirm\n",
        "print('We are using a ', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "We are using a  Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bis9NhstsAy1",
        "colab_type": "text"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWwnbN38CH9y",
        "colab_type": "code",
        "outputId": "cf25e642-ec52-4a66-d1cb-a897c3d02294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install wget\n",
        "# unzipping glue datasets\n",
        "!unzip 60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=0740926bda780cc7deb212404cad24d420d9f9f6b23483f7ceb49f733a34c2b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=88ab7fee296371535d953516a29e2efb27bc7be672d0e94873a8415065799d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Qe-kT2Cvhk",
        "colab_type": "code",
        "outputId": "0afad0af-1db9-4de2-f6a7-2f1894fd049a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# downloading datasets\n",
        "!python '/content/60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting SNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xEXBnaYCV1l",
        "colab_type": "text"
      },
      "source": [
        "-------------------\n",
        "### Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPb4WIEyCY2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "import re\n",
        "\n",
        "# function to read and convert MNLI dataset\n",
        "def read_data(filepath):\n",
        "  premises = []\n",
        "  hypotheses = []\n",
        "  labels = []\n",
        "  with open(filepath) as tsvfile:\n",
        "    reader = csv.reader(tsvfile, delimiter='\\n')\n",
        "    for row in reader:\n",
        "      line = re.split(r'\\t+', row[0])\n",
        "      premises.append(line[8])\n",
        "      hypotheses.append(line[9])\n",
        "      labels.append(line[10])\n",
        "  assert (len(premises) == len(hypotheses))\n",
        "  assert (len(premises) == len(labels))\n",
        "  \n",
        "  return premises, hypotheses, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82wcx5vNDWgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_premises, train_hypotheses, train_labels = read_data('/content/glue_data/MNLI/train.tsv')\n",
        "val_premises, val_hypotheses, val_labels = read_data('/content/glue_data/MNLI/dev_matched.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fREYmmicDu2s",
        "colab_type": "text"
      },
      "source": [
        "-----------------\n",
        "### Tokenizing\n",
        "\n",
        "- Add begining and end tokens\n",
        "- Pad & Truncate to single length\n",
        "- Add attention mask\n",
        "\n",
        "e.g:\n",
        "\n",
        "      For sequence pairs:\n",
        "      tokens:    [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "      type_ids:      0 0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "      attention_mask:1 1  1    1    1     1       1 1     1  1  1  1   1 1\n",
        "\n",
        "      source: https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L161\n",
        "\n",
        "  Note that all three of these will be padded with 0's to match the max sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s72vKnspDoVl",
        "colab_type": "code",
        "outputId": "711ae736-3e36-4cc8-86ba-3b37c76de0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "27ef93f71ab54f8fb636f6a6ef38442b",
            "8571c03ed4cb4eeb8fccd3b8d5562ee8",
            "1aa6a32b9d4248e5a073950f7a05582a",
            "94c7b29a235f4231a0e555c32f9a9ea9",
            "00a8e56294464f579638016d767f50bd",
            "e8e155bac072443f950c0a7e41cfdba1",
            "c2b99b8fd0b34c47a5835234477bd44c",
            "893f4052f9c74940bfd7d75ea649f123"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# loading bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27ef93f71ab54f8fb636f6a6ef38442b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrP_wbITD4GA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_sentences(premises, hypotheses, max_sequence_length):\n",
        "  # premises, hypothesis, and labels should all have same length\n",
        "  encoded_sentences = []\n",
        "  type_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for i in range(0,len(premises)):\n",
        "    # skip first line\n",
        "    if i == 0: continue\n",
        "    encoded_sentence = []\n",
        "    type_id = []\n",
        "    attention_mask = []\n",
        "\n",
        "    # encoding premise\n",
        "    encoded_premise = (tokenizer.convert_tokens_to_ids(tokenizer.tokenize(premises[i])))\n",
        "    # encoding hypothesis\n",
        "    encoded_hypothesis = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(hypotheses[i]))\n",
        "\n",
        "    # truncate if length is too large\n",
        "    # following heuristic in https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L557\n",
        "    # i.e. always truncate smaller one\n",
        "    if len(encoded_premise) + len(encoded_hypothesis) > max_sequence_length-3:\n",
        "      while True:\n",
        "        length = len(encoded_premise) + len(encoded_hypothesis)\n",
        "        if length <= max_sequence_length-3:\n",
        "          break\n",
        "        elif len(encoded_premise) > len(encoded_hypothesis):\n",
        "          encoded_premise.pop()\n",
        "        else:\n",
        "          encoded_hypothesis.pop()\n",
        "\n",
        "    # build vectors\n",
        "    encoded_sentence.append(101)  # [CLS] == 101\n",
        "    encoded_sentence += encoded_premise\n",
        "    encoded_sentence.append(102) # [SEP] == 101\n",
        "    type_id = list(np.zeros(len(encoded_sentence)))\n",
        "\n",
        "    encoded_sentence += encoded_hypothesis\n",
        "    encoded_sentence.append(102)\n",
        "    type_id += list(np.ones(len(encoded_hypothesis)+1))\n",
        "\n",
        "    assert len(encoded_sentence) == len(type_id)\n",
        "\n",
        "    # attention mask\n",
        "    attention_mask = list(np.ones(len(encoded_sentence)))\n",
        "\n",
        "    # pad to max_sequence_length\n",
        "    while len(encoded_sentence) < max_sequence_length:\n",
        "      encoded_sentence.append(0)\n",
        "      type_id.append(0)\n",
        "      attention_mask.append(0)\n",
        "\n",
        "    assert len(encoded_sentence) == max_sequence_length\n",
        "    assert len(type_id) == max_sequence_length\n",
        "    assert len(attention_mask) == max_sequence_length\n",
        "    \n",
        "    encoded_sentences.append(encoded_sentence)\n",
        "    type_ids.append(type_id)\n",
        "    attention_masks.append(attention_mask)\n",
        "\n",
        "  return encoded_sentences, type_ids, attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LYs2CfcI5oQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_labels(labels):\n",
        "  encoded_labels = []\n",
        "  for i in range(0, len(labels)):\n",
        "    # skip 'label1'\n",
        "    if i == 0: \n",
        "      continue\n",
        "    if labels[i] == 'entailment':\n",
        "      encoded_labels.append(0)\n",
        "    elif labels[i] == 'contradiction':\n",
        "      encoded_labels.append(1)\n",
        "    else:\n",
        "      encoded_labels.append(2)\n",
        "\n",
        "  return list(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqfNHRRTLPTs",
        "colab_type": "text"
      },
      "source": [
        "The following steps may take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDrbGY92JUxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs, train_ids, train_masks = tokenize_sentences(train_premises, train_hypotheses, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8BCgg4bJvBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_inputs, val_ids, val_masks = tokenize_sentences(val_premises, val_hypotheses, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItbscOUcLTb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = tokenize_labels(train_labels)\n",
        "val_labels = tokenize_labels(val_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDcsr5V5Lgbl",
        "colab_type": "text"
      },
      "source": [
        "We now need to convert these vectors to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCxAQZ25Ld0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoFkiV0RrHc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ids = torch.tensor(train_ids)\n",
        "val_ids = torch.tensor(val_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNDDJy-LrE16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYCvyJ0wq0-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2isE8XG6M6kr",
        "colab_type": "text"
      },
      "source": [
        "### Creating DataLoaders\n",
        "Python iterables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPl-T63H_Vz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WByoEH2g_WmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNK-gCIyMaL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create DataLoader for training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_ids, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "validation_data = TensorDataset(val_inputs, val_masks, val_ids, val_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT0XKAkoNuvw",
        "colab_type": "text"
      },
      "source": [
        "--------\n",
        "### Loading BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3LGpln7Nr75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PftAaXCgN7YM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 3,  \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "# run model on GPU\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AnO80ubOm5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "total_steps = len(train_dataloader)*epochs\n",
        "\n",
        "# create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T_eB7Q-PC03",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Again, this script is based off of https://mccormickml.com/2019/07/22/BERT-fine-tuning/#11-using-colab-gpu-for-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV50Nol2OoxH",
        "colab_type": "code",
        "outputId": "dcdd74b1-f500-40b9-8eb2-8a3cbeb70fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "seed = 72\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "  print('---------- Epoch %s ----------' % str(epoch))\n",
        "  # start clock\n",
        "  t0 = time.time()\n",
        "\n",
        "  # reset loss for epoch\n",
        "  total_loss = 0\n",
        "\n",
        "  # put model into training mode\n",
        "  model.train()\n",
        "\n",
        "  # for each batch of the training data\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "      time_elapsed = str(datetime.timedelta(seconds=int(round(time.time() - t0))))\n",
        "      print('\\t Batch %i of %i. Time elapsed: %s' % (step, len(train_dataloader), time_elapsed))\n",
        "    \n",
        "    # retrieve tensors from dataloader\n",
        "    # copy each to GPU using to(device)\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    sequence_ids = batch[2].to(device)\n",
        "    labels = batch[3].to(device)\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # perform forward pass\n",
        "    # the loss is returned\n",
        "    outputs = model(\n",
        "        input_ids = input_ids.long(),\n",
        "        attention_mask = attention_mask.long(),\n",
        "        token_type_ids = sequence_ids.long(),\n",
        "        labels = labels.long()\n",
        "        )\n",
        "    \n",
        "    loss = outputs[0]\n",
        "    total_loss += loss\n",
        "\n",
        "    # perform backward pass to calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0 to help prevent \"exploding gradients\" \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "  try:\n",
        "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "    output_dir = '/content/model_save/'\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "  except:\n",
        "    print('Saving Failed')\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "  loss_values.append(avg_train_loss)\n",
        "\n",
        "  print('--- Average Training Loss: %f' % avg_train_loss)\n",
        "\n",
        "  # Measure performance on validation set\n",
        "  t0 = time.time()\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "      # Unpack the inputs from dataloader\n",
        "      input_ids, attention_mask, sequence_ids, labels = batch\n",
        "\n",
        "      # no need for grad since evaluation\n",
        "      with torch.no_grad():        \n",
        "\n",
        "        outputs = model(input_ids = input_ids.long(),\n",
        "                          attention_mask = attention_mask.long(),\n",
        "                          token_type_ids = sequence_ids.long())\n",
        "          \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "          \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = np.sum(np.argmax(logits, axis=1).flatten() == label_ids.flatten())/len(label_ids)\n",
        "          \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "  try:\n",
        "    print(\"  Validation took: {:}\".format((datetime.timedelta(seconds=int(round(time.time() - t0)))))) \n",
        "  except:\n",
        "    continue\n",
        "\n",
        "print('Training complete.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- Epoch 0 ----------\n",
            "\t Batch 100 of 12272. Time elapsed: 0:01:13\n",
            "\t Batch 200 of 12272. Time elapsed: 0:02:23\n",
            "\t Batch 300 of 12272. Time elapsed: 0:03:34\n",
            "\t Batch 400 of 12272. Time elapsed: 0:04:44\n",
            "\t Batch 500 of 12272. Time elapsed: 0:05:55\n",
            "\t Batch 600 of 12272. Time elapsed: 0:07:06\n",
            "\t Batch 700 of 12272. Time elapsed: 0:08:16\n",
            "\t Batch 800 of 12272. Time elapsed: 0:09:27\n",
            "\t Batch 900 of 12272. Time elapsed: 0:10:38\n",
            "\t Batch 1000 of 12272. Time elapsed: 0:11:48\n",
            "\t Batch 1100 of 12272. Time elapsed: 0:12:59\n",
            "\t Batch 1200 of 12272. Time elapsed: 0:14:10\n",
            "\t Batch 1300 of 12272. Time elapsed: 0:15:20\n",
            "\t Batch 1400 of 12272. Time elapsed: 0:16:31\n",
            "\t Batch 1500 of 12272. Time elapsed: 0:17:41\n",
            "\t Batch 1600 of 12272. Time elapsed: 0:18:52\n",
            "\t Batch 1700 of 12272. Time elapsed: 0:20:03\n",
            "\t Batch 1800 of 12272. Time elapsed: 0:21:13\n",
            "\t Batch 1900 of 12272. Time elapsed: 0:22:24\n",
            "\t Batch 2000 of 12272. Time elapsed: 0:23:35\n",
            "\t Batch 2100 of 12272. Time elapsed: 0:24:45\n",
            "\t Batch 2200 of 12272. Time elapsed: 0:25:56\n",
            "\t Batch 2300 of 12272. Time elapsed: 0:27:07\n",
            "\t Batch 2400 of 12272. Time elapsed: 0:28:17\n",
            "\t Batch 2500 of 12272. Time elapsed: 0:29:28\n",
            "\t Batch 2600 of 12272. Time elapsed: 0:30:38\n",
            "\t Batch 2700 of 12272. Time elapsed: 0:31:49\n",
            "\t Batch 2800 of 12272. Time elapsed: 0:33:00\n",
            "\t Batch 2900 of 12272. Time elapsed: 0:34:10\n",
            "\t Batch 3000 of 12272. Time elapsed: 0:35:21\n",
            "\t Batch 3100 of 12272. Time elapsed: 0:36:32\n",
            "\t Batch 3200 of 12272. Time elapsed: 0:37:42\n",
            "\t Batch 3300 of 12272. Time elapsed: 0:38:53\n",
            "\t Batch 3400 of 12272. Time elapsed: 0:40:04\n",
            "\t Batch 3500 of 12272. Time elapsed: 0:41:14\n",
            "\t Batch 3600 of 12272. Time elapsed: 0:42:25\n",
            "\t Batch 3700 of 12272. Time elapsed: 0:43:36\n",
            "\t Batch 3800 of 12272. Time elapsed: 0:44:46\n",
            "\t Batch 3900 of 12272. Time elapsed: 0:45:57\n",
            "\t Batch 4000 of 12272. Time elapsed: 0:47:08\n",
            "\t Batch 4100 of 12272. Time elapsed: 0:48:18\n",
            "\t Batch 4200 of 12272. Time elapsed: 0:49:29\n",
            "\t Batch 4300 of 12272. Time elapsed: 0:50:40\n",
            "\t Batch 4400 of 12272. Time elapsed: 0:51:50\n",
            "\t Batch 4500 of 12272. Time elapsed: 0:53:01\n",
            "\t Batch 4600 of 12272. Time elapsed: 0:54:11\n",
            "\t Batch 4700 of 12272. Time elapsed: 0:55:22\n",
            "\t Batch 4800 of 12272. Time elapsed: 0:56:33\n",
            "\t Batch 4900 of 12272. Time elapsed: 0:57:43\n",
            "\t Batch 5000 of 12272. Time elapsed: 0:58:54\n",
            "\t Batch 5100 of 12272. Time elapsed: 1:00:05\n",
            "\t Batch 5200 of 12272. Time elapsed: 1:01:15\n",
            "\t Batch 5300 of 12272. Time elapsed: 1:02:26\n",
            "\t Batch 5400 of 12272. Time elapsed: 1:03:37\n",
            "\t Batch 5500 of 12272. Time elapsed: 1:04:47\n",
            "\t Batch 5600 of 12272. Time elapsed: 1:05:58\n",
            "\t Batch 5700 of 12272. Time elapsed: 1:07:09\n",
            "\t Batch 5800 of 12272. Time elapsed: 1:08:19\n",
            "\t Batch 5900 of 12272. Time elapsed: 1:09:30\n",
            "\t Batch 6000 of 12272. Time elapsed: 1:10:41\n",
            "\t Batch 6100 of 12272. Time elapsed: 1:11:51\n",
            "\t Batch 6200 of 12272. Time elapsed: 1:13:02\n",
            "\t Batch 6300 of 12272. Time elapsed: 1:14:13\n",
            "\t Batch 6400 of 12272. Time elapsed: 1:15:23\n",
            "\t Batch 6500 of 12272. Time elapsed: 1:16:34\n",
            "\t Batch 6600 of 12272. Time elapsed: 1:17:45\n",
            "\t Batch 6700 of 12272. Time elapsed: 1:18:55\n",
            "\t Batch 6800 of 12272. Time elapsed: 1:20:06\n",
            "\t Batch 6900 of 12272. Time elapsed: 1:21:17\n",
            "\t Batch 7000 of 12272. Time elapsed: 1:22:27\n",
            "\t Batch 7100 of 12272. Time elapsed: 1:23:38\n",
            "\t Batch 7200 of 12272. Time elapsed: 1:24:48\n",
            "\t Batch 7300 of 12272. Time elapsed: 1:25:59\n",
            "\t Batch 7400 of 12272. Time elapsed: 1:27:10\n",
            "\t Batch 7500 of 12272. Time elapsed: 1:28:20\n",
            "\t Batch 7600 of 12272. Time elapsed: 1:29:31\n",
            "\t Batch 7700 of 12272. Time elapsed: 1:30:42\n",
            "\t Batch 7800 of 12272. Time elapsed: 1:31:52\n",
            "\t Batch 7900 of 12272. Time elapsed: 1:33:03\n",
            "\t Batch 8000 of 12272. Time elapsed: 1:34:14\n",
            "\t Batch 8100 of 12272. Time elapsed: 1:35:24\n",
            "\t Batch 8200 of 12272. Time elapsed: 1:36:35\n",
            "\t Batch 8300 of 12272. Time elapsed: 1:37:46\n",
            "\t Batch 8400 of 12272. Time elapsed: 1:38:56\n",
            "\t Batch 8500 of 12272. Time elapsed: 1:40:07\n",
            "\t Batch 8600 of 12272. Time elapsed: 1:41:18\n",
            "\t Batch 8700 of 12272. Time elapsed: 1:42:28\n",
            "\t Batch 8800 of 12272. Time elapsed: 1:43:39\n",
            "\t Batch 8900 of 12272. Time elapsed: 1:44:50\n",
            "\t Batch 9000 of 12272. Time elapsed: 1:46:00\n",
            "\t Batch 9100 of 12272. Time elapsed: 1:47:11\n",
            "\t Batch 9200 of 12272. Time elapsed: 1:48:22\n",
            "\t Batch 9300 of 12272. Time elapsed: 1:49:32\n",
            "\t Batch 9400 of 12272. Time elapsed: 1:50:43\n",
            "\t Batch 9500 of 12272. Time elapsed: 1:51:54\n",
            "\t Batch 9600 of 12272. Time elapsed: 1:53:04\n",
            "\t Batch 9700 of 12272. Time elapsed: 1:54:15\n",
            "\t Batch 9800 of 12272. Time elapsed: 1:55:26\n",
            "\t Batch 9900 of 12272. Time elapsed: 1:56:36\n",
            "\t Batch 10000 of 12272. Time elapsed: 1:57:47\n",
            "\t Batch 10100 of 12272. Time elapsed: 1:58:58\n",
            "\t Batch 10200 of 12272. Time elapsed: 2:00:08\n",
            "\t Batch 10300 of 12272. Time elapsed: 2:01:19\n",
            "\t Batch 10400 of 12272. Time elapsed: 2:02:30\n",
            "\t Batch 10500 of 12272. Time elapsed: 2:03:40\n",
            "\t Batch 10600 of 12272. Time elapsed: 2:04:51\n",
            "\t Batch 10700 of 12272. Time elapsed: 2:06:02\n",
            "\t Batch 10800 of 12272. Time elapsed: 2:07:12\n",
            "\t Batch 10900 of 12272. Time elapsed: 2:08:23\n",
            "\t Batch 11000 of 12272. Time elapsed: 2:09:34\n",
            "\t Batch 11100 of 12272. Time elapsed: 2:10:44\n",
            "\t Batch 11200 of 12272. Time elapsed: 2:11:55\n",
            "\t Batch 11300 of 12272. Time elapsed: 2:13:06\n",
            "\t Batch 11400 of 12272. Time elapsed: 2:14:16\n",
            "\t Batch 11500 of 12272. Time elapsed: 2:15:27\n",
            "\t Batch 11600 of 12272. Time elapsed: 2:16:37\n",
            "\t Batch 11700 of 12272. Time elapsed: 2:17:48\n",
            "\t Batch 11800 of 12272. Time elapsed: 2:18:59\n",
            "\t Batch 11900 of 12272. Time elapsed: 2:20:09\n",
            "\t Batch 12000 of 12272. Time elapsed: 2:21:20\n",
            "\t Batch 12100 of 12272. Time elapsed: 2:22:31\n",
            "\t Batch 12200 of 12272. Time elapsed: 2:23:41\n",
            "Saving model to /content/model_save/\n",
            "saving failed\n",
            "--- Average Training Loss: 0.522717\n",
            "---------- Epoch 1 ----------\n",
            "\t Batch 100 of 12272. Time elapsed: 0:01:11\n",
            "\t Batch 200 of 12272. Time elapsed: 0:02:21\n",
            "\t Batch 300 of 12272. Time elapsed: 0:03:32\n",
            "\t Batch 400 of 12272. Time elapsed: 0:04:43\n",
            "\t Batch 500 of 12272. Time elapsed: 0:05:53\n",
            "\t Batch 600 of 12272. Time elapsed: 0:07:04\n",
            "\t Batch 700 of 12272. Time elapsed: 0:08:15\n",
            "\t Batch 800 of 12272. Time elapsed: 0:09:25\n",
            "\t Batch 900 of 12272. Time elapsed: 0:10:36\n",
            "\t Batch 1000 of 12272. Time elapsed: 0:11:47\n",
            "\t Batch 1100 of 12272. Time elapsed: 0:12:57\n",
            "\t Batch 1200 of 12272. Time elapsed: 0:14:08\n",
            "\t Batch 1300 of 12272. Time elapsed: 0:15:19\n",
            "\t Batch 1400 of 12272. Time elapsed: 0:16:29\n",
            "\t Batch 1500 of 12272. Time elapsed: 0:17:40\n",
            "\t Batch 1600 of 12272. Time elapsed: 0:18:51\n",
            "\t Batch 1700 of 12272. Time elapsed: 0:20:01\n",
            "\t Batch 1800 of 12272. Time elapsed: 0:21:12\n",
            "\t Batch 1900 of 12272. Time elapsed: 0:22:23\n",
            "\t Batch 2000 of 12272. Time elapsed: 0:23:33\n",
            "\t Batch 2100 of 12272. Time elapsed: 0:24:44\n",
            "\t Batch 2200 of 12272. Time elapsed: 0:25:54\n",
            "\t Batch 2300 of 12272. Time elapsed: 0:27:05\n",
            "\t Batch 2400 of 12272. Time elapsed: 0:28:16\n",
            "\t Batch 2500 of 12272. Time elapsed: 0:29:26\n",
            "\t Batch 2600 of 12272. Time elapsed: 0:30:37\n",
            "\t Batch 2700 of 12272. Time elapsed: 0:31:48\n",
            "\t Batch 2800 of 12272. Time elapsed: 0:32:58\n",
            "\t Batch 2900 of 12272. Time elapsed: 0:34:09\n",
            "\t Batch 3000 of 12272. Time elapsed: 0:35:20\n",
            "\t Batch 3100 of 12272. Time elapsed: 0:36:31\n",
            "\t Batch 3200 of 12272. Time elapsed: 0:37:41\n",
            "\t Batch 3300 of 12272. Time elapsed: 0:38:52\n",
            "\t Batch 3400 of 12272. Time elapsed: 0:40:02\n",
            "\t Batch 3500 of 12272. Time elapsed: 0:41:13\n",
            "\t Batch 3600 of 12272. Time elapsed: 0:42:24\n",
            "\t Batch 3700 of 12272. Time elapsed: 0:43:34\n",
            "\t Batch 3800 of 12272. Time elapsed: 0:44:45\n",
            "\t Batch 3900 of 12272. Time elapsed: 0:45:56\n",
            "\t Batch 4000 of 12272. Time elapsed: 0:47:06\n",
            "\t Batch 4100 of 12272. Time elapsed: 0:48:17\n",
            "\t Batch 4200 of 12272. Time elapsed: 0:49:28\n",
            "\t Batch 4300 of 12272. Time elapsed: 0:50:38\n",
            "\t Batch 4400 of 12272. Time elapsed: 0:51:49\n",
            "\t Batch 4500 of 12272. Time elapsed: 0:53:00\n",
            "\t Batch 4600 of 12272. Time elapsed: 0:54:10\n",
            "\t Batch 4700 of 12272. Time elapsed: 0:55:21\n",
            "\t Batch 4800 of 12272. Time elapsed: 0:56:32\n",
            "\t Batch 4900 of 12272. Time elapsed: 0:57:42\n",
            "\t Batch 5000 of 12272. Time elapsed: 0:58:53\n",
            "\t Batch 5100 of 12272. Time elapsed: 1:00:04\n",
            "\t Batch 5200 of 12272. Time elapsed: 1:01:14\n",
            "\t Batch 5300 of 12272. Time elapsed: 1:02:25\n",
            "\t Batch 5400 of 12272. Time elapsed: 1:03:36\n",
            "\t Batch 5500 of 12272. Time elapsed: 1:04:46\n",
            "\t Batch 5600 of 12272. Time elapsed: 1:05:57\n",
            "\t Batch 5700 of 12272. Time elapsed: 1:07:08\n",
            "\t Batch 5800 of 12272. Time elapsed: 1:08:18\n",
            "\t Batch 5900 of 12272. Time elapsed: 1:09:29\n",
            "\t Batch 6000 of 12272. Time elapsed: 1:10:40\n",
            "\t Batch 6100 of 12272. Time elapsed: 1:11:50\n",
            "\t Batch 6200 of 12272. Time elapsed: 1:13:01\n",
            "\t Batch 6300 of 12272. Time elapsed: 1:14:12\n",
            "\t Batch 6400 of 12272. Time elapsed: 1:15:22\n",
            "\t Batch 6500 of 12272. Time elapsed: 1:16:33\n",
            "\t Batch 6600 of 12272. Time elapsed: 1:17:44\n",
            "\t Batch 6700 of 12272. Time elapsed: 1:18:54\n",
            "\t Batch 6800 of 12272. Time elapsed: 1:20:05\n",
            "\t Batch 6900 of 12272. Time elapsed: 1:21:16\n",
            "\t Batch 7000 of 12272. Time elapsed: 1:22:26\n",
            "\t Batch 7100 of 12272. Time elapsed: 1:23:37\n",
            "\t Batch 7200 of 12272. Time elapsed: 1:24:48\n",
            "\t Batch 7300 of 12272. Time elapsed: 1:25:58\n",
            "\t Batch 7400 of 12272. Time elapsed: 1:27:09\n",
            "\t Batch 7500 of 12272. Time elapsed: 1:28:20\n",
            "\t Batch 7600 of 12272. Time elapsed: 1:29:30\n",
            "\t Batch 7700 of 12272. Time elapsed: 1:30:41\n",
            "\t Batch 7800 of 12272. Time elapsed: 1:31:51\n",
            "\t Batch 7900 of 12272. Time elapsed: 1:33:02\n",
            "\t Batch 8000 of 12272. Time elapsed: 1:34:13\n",
            "\t Batch 8100 of 12272. Time elapsed: 1:35:24\n",
            "\t Batch 8200 of 12272. Time elapsed: 1:36:34\n",
            "\t Batch 8300 of 12272. Time elapsed: 1:37:45\n",
            "\t Batch 8400 of 12272. Time elapsed: 1:38:56\n",
            "\t Batch 8500 of 12272. Time elapsed: 1:40:06\n",
            "\t Batch 8600 of 12272. Time elapsed: 1:41:17\n",
            "\t Batch 8700 of 12272. Time elapsed: 1:42:28\n",
            "\t Batch 8800 of 12272. Time elapsed: 1:43:38\n",
            "\t Batch 8900 of 12272. Time elapsed: 1:44:49\n",
            "\t Batch 9000 of 12272. Time elapsed: 1:46:00\n",
            "\t Batch 9100 of 12272. Time elapsed: 1:47:10\n",
            "\t Batch 9200 of 12272. Time elapsed: 1:48:21\n",
            "\t Batch 9300 of 12272. Time elapsed: 1:49:32\n",
            "\t Batch 9400 of 12272. Time elapsed: 1:50:42\n",
            "\t Batch 9500 of 12272. Time elapsed: 1:51:53\n",
            "\t Batch 9600 of 12272. Time elapsed: 1:53:03\n",
            "\t Batch 9700 of 12272. Time elapsed: 1:54:14\n",
            "\t Batch 9800 of 12272. Time elapsed: 1:55:25\n",
            "\t Batch 9900 of 12272. Time elapsed: 1:56:35\n",
            "\t Batch 10000 of 12272. Time elapsed: 1:57:46\n",
            "\t Batch 10100 of 12272. Time elapsed: 1:58:57\n",
            "\t Batch 10200 of 12272. Time elapsed: 2:00:07\n",
            "\t Batch 10300 of 12272. Time elapsed: 2:01:18\n",
            "\t Batch 10400 of 12272. Time elapsed: 2:02:29\n",
            "\t Batch 10500 of 12272. Time elapsed: 2:03:39\n",
            "\t Batch 10600 of 12272. Time elapsed: 2:04:50\n",
            "\t Batch 10700 of 12272. Time elapsed: 2:06:01\n",
            "\t Batch 10800 of 12272. Time elapsed: 2:07:11\n",
            "\t Batch 10900 of 12272. Time elapsed: 2:08:22\n",
            "\t Batch 11000 of 12272. Time elapsed: 2:09:33\n",
            "\t Batch 11100 of 12272. Time elapsed: 2:10:43\n",
            "\t Batch 11200 of 12272. Time elapsed: 2:11:54\n",
            "\t Batch 11300 of 12272. Time elapsed: 2:13:05\n",
            "\t Batch 11400 of 12272. Time elapsed: 2:14:15\n",
            "\t Batch 11500 of 12272. Time elapsed: 2:15:26\n",
            "\t Batch 11600 of 12272. Time elapsed: 2:16:36\n",
            "\t Batch 11700 of 12272. Time elapsed: 2:17:47\n",
            "\t Batch 11800 of 12272. Time elapsed: 2:18:58\n",
            "\t Batch 11900 of 12272. Time elapsed: 2:20:08\n",
            "\t Batch 12000 of 12272. Time elapsed: 2:21:19\n",
            "\t Batch 12100 of 12272. Time elapsed: 2:22:30\n",
            "\t Batch 12200 of 12272. Time elapsed: 2:23:40\n",
            "Saving model to /content/model_save/\n",
            "saving failed\n",
            "--- Average Training Loss: 0.342380\n",
            "---------- Epoch 2 ----------\n",
            "\t Batch 100 of 12272. Time elapsed: 0:01:11\n",
            "\t Batch 200 of 12272. Time elapsed: 0:02:21\n",
            "\t Batch 300 of 12272. Time elapsed: 0:03:32\n",
            "\t Batch 400 of 12272. Time elapsed: 0:04:43\n",
            "\t Batch 500 of 12272. Time elapsed: 0:05:53\n",
            "\t Batch 600 of 12272. Time elapsed: 0:07:04\n",
            "\t Batch 700 of 12272. Time elapsed: 0:08:15\n",
            "\t Batch 800 of 12272. Time elapsed: 0:09:25\n",
            "\t Batch 900 of 12272. Time elapsed: 0:10:36\n",
            "\t Batch 1000 of 12272. Time elapsed: 0:11:47\n",
            "\t Batch 1100 of 12272. Time elapsed: 0:12:57\n",
            "\t Batch 1200 of 12272. Time elapsed: 0:14:08\n",
            "\t Batch 1300 of 12272. Time elapsed: 0:15:19\n",
            "\t Batch 1400 of 12272. Time elapsed: 0:16:29\n",
            "\t Batch 1500 of 12272. Time elapsed: 0:17:40\n",
            "\t Batch 1600 of 12272. Time elapsed: 0:18:50\n",
            "\t Batch 1700 of 12272. Time elapsed: 0:20:01\n",
            "\t Batch 1800 of 12272. Time elapsed: 0:21:12\n",
            "\t Batch 1900 of 12272. Time elapsed: 0:22:23\n",
            "\t Batch 2000 of 12272. Time elapsed: 0:23:33\n",
            "\t Batch 2100 of 12272. Time elapsed: 0:24:44\n",
            "\t Batch 2200 of 12272. Time elapsed: 0:25:55\n",
            "\t Batch 2300 of 12272. Time elapsed: 0:27:05\n",
            "\t Batch 2400 of 12272. Time elapsed: 0:28:16\n",
            "\t Batch 2500 of 12272. Time elapsed: 0:29:26\n",
            "\t Batch 2600 of 12272. Time elapsed: 0:30:37\n",
            "\t Batch 2700 of 12272. Time elapsed: 0:31:48\n",
            "\t Batch 2800 of 12272. Time elapsed: 0:32:59\n",
            "\t Batch 2900 of 12272. Time elapsed: 0:34:09\n",
            "\t Batch 3000 of 12272. Time elapsed: 0:35:20\n",
            "\t Batch 3100 of 12272. Time elapsed: 0:36:30\n",
            "\t Batch 3200 of 12272. Time elapsed: 0:37:41\n",
            "\t Batch 3300 of 12272. Time elapsed: 0:38:52\n",
            "\t Batch 3400 of 12272. Time elapsed: 0:40:02\n",
            "\t Batch 3500 of 12272. Time elapsed: 0:41:13\n",
            "\t Batch 3600 of 12272. Time elapsed: 0:42:24\n",
            "\t Batch 3700 of 12272. Time elapsed: 0:43:35\n",
            "\t Batch 3800 of 12272. Time elapsed: 0:44:45\n",
            "\t Batch 3900 of 12272. Time elapsed: 0:45:56\n",
            "\t Batch 4000 of 12272. Time elapsed: 0:47:07\n",
            "\t Batch 4100 of 12272. Time elapsed: 0:48:17\n",
            "\t Batch 4200 of 12272. Time elapsed: 0:49:28\n",
            "\t Batch 4300 of 12272. Time elapsed: 0:50:39\n",
            "\t Batch 4400 of 12272. Time elapsed: 0:51:49\n",
            "\t Batch 4500 of 12272. Time elapsed: 0:53:00\n",
            "\t Batch 4600 of 12272. Time elapsed: 0:54:10\n",
            "\t Batch 4700 of 12272. Time elapsed: 0:55:21\n",
            "\t Batch 4800 of 12272. Time elapsed: 0:56:32\n",
            "\t Batch 4900 of 12272. Time elapsed: 0:57:42\n",
            "\t Batch 5000 of 12272. Time elapsed: 0:58:53\n",
            "\t Batch 5100 of 12272. Time elapsed: 1:00:04\n",
            "\t Batch 5200 of 12272. Time elapsed: 1:01:14\n",
            "\t Batch 5300 of 12272. Time elapsed: 1:02:25\n",
            "\t Batch 5400 of 12272. Time elapsed: 1:03:36\n",
            "\t Batch 5500 of 12272. Time elapsed: 1:04:46\n",
            "\t Batch 5600 of 12272. Time elapsed: 1:05:57\n",
            "\t Batch 5700 of 12272. Time elapsed: 1:07:08\n",
            "\t Batch 5800 of 12272. Time elapsed: 1:08:18\n",
            "\t Batch 5900 of 12272. Time elapsed: 1:09:29\n",
            "\t Batch 6000 of 12272. Time elapsed: 1:10:40\n",
            "\t Batch 6100 of 12272. Time elapsed: 1:11:51\n",
            "\t Batch 6200 of 12272. Time elapsed: 1:13:01\n",
            "\t Batch 6300 of 12272. Time elapsed: 1:14:12\n",
            "\t Batch 6400 of 12272. Time elapsed: 1:15:23\n",
            "\t Batch 6500 of 12272. Time elapsed: 1:16:33\n",
            "\t Batch 6600 of 12272. Time elapsed: 1:17:44\n",
            "\t Batch 6700 of 12272. Time elapsed: 1:18:55\n",
            "\t Batch 6800 of 12272. Time elapsed: 1:20:05\n",
            "\t Batch 6900 of 12272. Time elapsed: 1:21:16\n",
            "\t Batch 7000 of 12272. Time elapsed: 1:22:27\n",
            "\t Batch 7100 of 12272. Time elapsed: 1:23:37\n",
            "\t Batch 7200 of 12272. Time elapsed: 1:24:48\n",
            "\t Batch 7300 of 12272. Time elapsed: 1:25:58\n",
            "\t Batch 7400 of 12272. Time elapsed: 1:27:09\n",
            "\t Batch 7500 of 12272. Time elapsed: 1:28:20\n",
            "\t Batch 7600 of 12272. Time elapsed: 1:29:30\n",
            "\t Batch 7700 of 12272. Time elapsed: 1:30:41\n",
            "\t Batch 7800 of 12272. Time elapsed: 1:31:52\n",
            "\t Batch 7900 of 12272. Time elapsed: 1:33:02\n",
            "\t Batch 8000 of 12272. Time elapsed: 1:34:13\n",
            "\t Batch 8100 of 12272. Time elapsed: 1:35:24\n",
            "\t Batch 8200 of 12272. Time elapsed: 1:36:34\n",
            "\t Batch 8300 of 12272. Time elapsed: 1:37:45\n",
            "\t Batch 8400 of 12272. Time elapsed: 1:38:56\n",
            "\t Batch 8500 of 12272. Time elapsed: 1:40:06\n",
            "\t Batch 8600 of 12272. Time elapsed: 1:41:17\n",
            "\t Batch 8700 of 12272. Time elapsed: 1:42:28\n",
            "\t Batch 8800 of 12272. Time elapsed: 1:43:38\n",
            "\t Batch 8900 of 12272. Time elapsed: 1:44:49\n",
            "\t Batch 9000 of 12272. Time elapsed: 1:46:00\n",
            "\t Batch 9100 of 12272. Time elapsed: 1:47:10\n",
            "\t Batch 9200 of 12272. Time elapsed: 1:48:21\n",
            "\t Batch 9300 of 12272. Time elapsed: 1:49:32\n",
            "\t Batch 9400 of 12272. Time elapsed: 1:50:42\n",
            "\t Batch 9500 of 12272. Time elapsed: 1:51:53\n",
            "\t Batch 9600 of 12272. Time elapsed: 1:53:04\n",
            "\t Batch 9700 of 12272. Time elapsed: 1:54:14\n",
            "\t Batch 9800 of 12272. Time elapsed: 1:55:25\n",
            "\t Batch 9900 of 12272. Time elapsed: 1:56:36\n",
            "\t Batch 10000 of 12272. Time elapsed: 1:57:46\n",
            "\t Batch 10100 of 12272. Time elapsed: 1:58:57\n",
            "\t Batch 10200 of 12272. Time elapsed: 2:00:08\n",
            "\t Batch 10300 of 12272. Time elapsed: 2:01:18\n",
            "\t Batch 10400 of 12272. Time elapsed: 2:02:29\n",
            "\t Batch 10500 of 12272. Time elapsed: 2:03:39\n",
            "\t Batch 10600 of 12272. Time elapsed: 2:04:50\n",
            "\t Batch 10700 of 12272. Time elapsed: 2:06:01\n",
            "\t Batch 10800 of 12272. Time elapsed: 2:07:11\n",
            "\t Batch 10900 of 12272. Time elapsed: 2:08:22\n",
            "\t Batch 11000 of 12272. Time elapsed: 2:09:33\n",
            "\t Batch 11100 of 12272. Time elapsed: 2:10:43\n",
            "\t Batch 11200 of 12272. Time elapsed: 2:11:54\n",
            "\t Batch 11300 of 12272. Time elapsed: 2:13:04\n",
            "\t Batch 11400 of 12272. Time elapsed: 2:14:15\n",
            "\t Batch 11500 of 12272. Time elapsed: 2:15:26\n",
            "\t Batch 11600 of 12272. Time elapsed: 2:16:36\n",
            "\t Batch 11700 of 12272. Time elapsed: 2:17:47\n",
            "\t Batch 11800 of 12272. Time elapsed: 2:18:58\n",
            "\t Batch 11900 of 12272. Time elapsed: 2:20:08\n",
            "\t Batch 12000 of 12272. Time elapsed: 2:21:19\n",
            "\t Batch 12100 of 12272. Time elapsed: 2:22:30\n",
            "\t Batch 12200 of 12272. Time elapsed: 2:23:40\n",
            "Saving model to /content/model_save/\n",
            "saving failed\n",
            "--- Average Training Loss: 0.236554\n",
            "Training complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgrFBIYv_lvi",
        "colab_type": "code",
        "outputId": "114c462f-fd3a-41ab-ad94-2c105df1d0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in validation_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "      # Unpack the inputs from dataloader\n",
        "    input_ids, attention_mask, sequence_ids, labels = batch\n",
        "\n",
        "      # no need for grad since evaluation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "      outputs = model(input_ids = input_ids.long(),\n",
        "                          attention_mask = attention_mask.long(),\n",
        "                          token_type_ids = sequence_ids.long())\n",
        "          \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "      logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "          \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "      tmp_eval_accuracy = np.sum(np.argmax(logits, axis=1).flatten() == label_ids.flatten())/len(label_ids)\n",
        "          \n",
        "        # Accumulate the total accuracy.\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM83oivD7av6",
        "colab_type": "text"
      },
      "source": [
        "----------\n",
        "### Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PjGfmdbbhZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iYdFAhl-oB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since no labels are provided in the data set, we must redefine the read_data function\n",
        "def read_test_data(filepath):\n",
        "  premises = []\n",
        "  hypotheses = []\n",
        "  pair_ids = []\n",
        "  with open(filepath) as tsvfile:\n",
        "    first = True\n",
        "    reader = csv.reader(tsvfile, delimiter='\\n')\n",
        "    for row in reader:\n",
        "      line = re.split(r'\\t+', row[0])\n",
        "      if first:\n",
        "        first = False\n",
        "        premises.append(line[8])\n",
        "        hypotheses.append(line[9])\n",
        "        continue\n",
        "      premises.append(line[8])\n",
        "      hypotheses.append(line[9])\n",
        "      pair_ids.append(int(line[2]))\n",
        "  assert (len(premises) == len(hypotheses))\n",
        "  \n",
        "  return premises, hypotheses, pair_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8dbiyq5Auif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjsYBuDTh13-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_premises, test_hypotheses, test_pair_ids = read_test_data('/content/glue_data/MNLI/test_matched.tsv')\n",
        "test_inputs, test_ids, test_masks = tokenize_sentences(test_premises, test_hypotheses, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqHRyl_Q7xGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "test_pair_ids = torch.tensor(test_pair_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq5qQ4Ed8G4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ZGsprQ8Jn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = TensorDataset(test_inputs, test_masks, test_ids, test_pair_ids)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpQOOErh8gGW",
        "colab_type": "code",
        "outputId": "018f2ea2-6fde-41d6-fc38-c361b41fcd62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = []\n",
        "pair_ids = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "    # Unpack the inputs from dataloader\n",
        "    input_ids, attention_mask, sequence_ids, batch_pair_ids = batch\n",
        "\n",
        "    # no need for grad since evaluation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "      outputs = model(input_ids = input_ids.long(),\n",
        "                          attention_mask = attention_mask.long(),\n",
        "                          token_type_ids = sequence_ids.long())\n",
        "          \n",
        "      logits = outputs[0]\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      batch_pair_ids = batch_pair_ids.to('cpu').numpy()\n",
        "\n",
        "      for i in range(0,len(logits)): \n",
        "        predictions.append(logits[i])\n",
        "        pair_ids.append(batch_pair_ids[i])\n",
        "\n",
        "print('---- Testing Completed ----')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Testing Completed ----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Frd2zzJV4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_predictions = []\n",
        "for prediction in predictions:\n",
        "  label_index = np.argmax(prediction)\n",
        "  if label_index == 0:\n",
        "    label_predictions.append('entailment')\n",
        "  elif label_index == 1:\n",
        "    label_predictions.append('contradiction')\n",
        "  else:\n",
        "    label_predictions.append('neutral')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IBgYPF8DCm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output to CSV file to submit in kaggle competition\n",
        "df = pd.DataFrame()\n",
        "df['pairID'] = pair_ids\n",
        "df['gold_label'] = label_predictions\n",
        "df.to_csv('predictions.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUL4bSILEiPk",
        "colab_type": "code",
        "outputId": "cff85ca9-d4c6-4db9-f4f8-71f35ee9b40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pairID</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>55615</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85595</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42972</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>133689</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>108690</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9791</th>\n",
              "      <td>113013</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9792</th>\n",
              "      <td>14918</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9793</th>\n",
              "      <td>101150</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9794</th>\n",
              "      <td>69735</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9795</th>\n",
              "      <td>137387</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9796 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      pairID     gold_label\n",
              "0      55615        neutral\n",
              "1      85595        neutral\n",
              "2      42972     entailment\n",
              "3     133689  contradiction\n",
              "4     108690        neutral\n",
              "...      ...            ...\n",
              "9791  113013     entailment\n",
              "9792   14918        neutral\n",
              "9793  101150     entailment\n",
              "9794   69735  contradiction\n",
              "9795  137387        neutral\n",
              "\n",
              "[9796 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9tsVQ7cEjAs",
        "colab_type": "code",
        "outputId": "194d39d0-da36-44d3-e2a4-e3d30b4e0bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "output_dir = '/content/model_save_final/'\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    #model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/model_save_final/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/model_save_final/vocab.txt',\n",
              " '/content/model_save_final/special_tokens_map.json',\n",
              " '/content/model_save_final/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaA8QbGJSG06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_convert_hans(filepath): \n",
        "  premises = []\n",
        "  hypotheses = []\n",
        "  pairIDs = []\n",
        "  gold_labels = []\n",
        "  first_line = True\n",
        "  with open(filepath) as file:\n",
        "    for fline in file:\n",
        "      line = re.split(r'\\t+', fline)\n",
        "      if first_line == True:\n",
        "        first_line = False\n",
        "        premises.append(line[5])\n",
        "        hypotheses.append(line[6])\n",
        "        continue\n",
        "      pairIDs.append(int(re.sub('ex', '', line[7])))\n",
        "      gold_labels.append(line[0])\n",
        "      premises.append(line[5])\n",
        "      hypotheses.append(line[6])\n",
        "    \n",
        "    #assert(len(pairIDs) == len(premises))\n",
        "    assert(len(premises) == len(hypotheses))\n",
        "    assert(len(pairIDs) == len(gold_labels))\n",
        "\n",
        "    return premises, hypotheses, pairIDs, gold_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQTAUazKTyrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hans_premises, hans_hypotheses, hans_pairIDs, hans_labels = read_and_convert_hans('/content/heuristics_evaluation_set.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZW1sgmWUKHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs, test_ids, test_masks = tokenize_sentences(hans_premises, hans_hypotheses, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxmqyIUhUiYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "hans_pairIDs = torch.tensor(hans_pairIDs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj57SGXeUpzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = TensorDataset(test_inputs, test_masks, test_ids, hans_pairIDs)\n",
        "#test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfaRHORaUuZS",
        "colab_type": "code",
        "outputId": "ea1052bb-774a-4153-8da6-3e425beef682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = []\n",
        "pair_ids = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "    # Unpack the inputs from dataloader\n",
        "    input_ids, attention_mask, sequence_ids, batch_pair_ids = batch\n",
        "\n",
        "    # no need for grad since evaluation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "      outputs = model(input_ids = input_ids.long(),\n",
        "                          attention_mask = attention_mask.long(),\n",
        "                          token_type_ids = sequence_ids.long())\n",
        "          \n",
        "      logits = outputs[0]\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      batch_pair_ids = batch_pair_ids.to('cpu').numpy()\n",
        "\n",
        "      for i in range(0,len(logits)): \n",
        "        predictions.append(logits[i])\n",
        "        pair_ids.append('ex' + str(batch_pair_ids[i]))\n",
        "\n",
        "print('---- Testing Completed ----')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Testing Completed ----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8bVt5IMWApN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output to CSV file to submit in kaggle competition\n",
        "df = pd.DataFrame()\n",
        "df['pairID'] = pair_ids\n",
        "df['gold_label'] = predictions\n",
        "df.to_csv('hans_predictions.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkBaocOcXQOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('hans_predictions.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcGayZvNXQpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_predictions = []\n",
        "for prediction in predictions:\n",
        "  label_index = np.argmax(prediction)\n",
        "  if label_index == 0:\n",
        "    label_predictions.append('entailment')\n",
        "  else:\n",
        "    label_predictions.append('non-entailment')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46ULpgXaYKt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['pairID'] = pair_ids\n",
        "df['gold_label'] = label_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQqA1BpcYQPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('hans_predictions_with_labels.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IJUz68NYSyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('predictions_with_labels.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dewbjvX4HusA",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "Evaluating Results on HANS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej4SXikNjR1V",
        "colab_type": "code",
        "outputId": "bc9c58f9-3819-40b8-c218-314a6bd63a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python evaluate_heur_output.py hans_predictions_with_labels.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heuristic entailed results:\n",
            "lexical_overlap: 0.9588\n",
            "subsequence: 0.9852\n",
            "constituent: 0.992\n",
            "\n",
            "Heuristic non-entailed results:\n",
            "lexical_overlap: 0.4464\n",
            "subsequence: 0.0958\n",
            "constituent: 0.1524\n",
            "\n",
            "Subcase results:\n",
            "ln_subject/object_swap: 0.425\n",
            "ln_preposition: 0.616\n",
            "ln_relative_clause: 0.484\n",
            "ln_passive: 0.022\n",
            "ln_conjunction: 0.685\n",
            "le_relative_clause: 0.963\n",
            "le_around_prepositional_phrase: 0.999\n",
            "le_around_relative_clause: 0.988\n",
            "le_conjunction: 0.845\n",
            "le_passive: 0.999\n",
            "sn_NP/S: 0.028\n",
            "sn_PP_on_subject: 0.247\n",
            "sn_relative_clause_on_subject: 0.132\n",
            "sn_past_participle: 0.004\n",
            "sn_NP/Z: 0.068\n",
            "se_conjunction: 0.933\n",
            "se_adjective: 1.0\n",
            "se_understood_object: 0.995\n",
            "se_relative_clause_on_obj: 0.998\n",
            "se_PP_on_obj: 1.0\n",
            "cn_embedded_under_if: 0.398\n",
            "cn_after_if_clause: 0.03\n",
            "cn_embedded_under_verb: 0.201\n",
            "cn_disjunction: 0.022\n",
            "cn_adverb: 0.111\n",
            "ce_embedded_under_since: 0.968\n",
            "ce_after_since_clause: 1.0\n",
            "ce_embedded_under_verb: 0.992\n",
            "ce_conjunction: 1.0\n",
            "ce_adverb: 1.0\n",
            "\n",
            "Template results:\n",
            "temp1: 0.425\n",
            "temp5: 0.3885350318471338\n",
            "temp7: 0.7216494845360825\n",
            "temp3: 0.630057803468208\n",
            "temp2: 0.7175141242937854\n",
            "temp4: 0.7302631578947368\n",
            "temp6: 0.46258503401360546\n",
            "temp11: 0.7261904761904762\n",
            "temp9: 0.42574257425742573\n",
            "temp15: 0.38\n",
            "temp16: 0.7\n",
            "temp10: 0.6097560975609756\n",
            "temp8: 0.5797101449275363\n",
            "temp18: 0.532608695652174\n",
            "temp12: 0.3333333333333333\n",
            "temp14: 0.2716049382716049\n",
            "temp19: 0.5232558139534884\n",
            "temp13: 0.4342105263157895\n",
            "temp17: 0.29411764705882354\n",
            "temp21: 0.028282828282828285\n",
            "temp20: 0.015841584158415842\n",
            "temp22: 0.7161016949152542\n",
            "temp25: 0.6798245614035088\n",
            "temp24: 0.6426116838487973\n",
            "temp23: 0.710204081632653\n",
            "temp28: 0.9765625\n",
            "temp26: 0.9661654135338346\n",
            "temp29: 0.9565217391304348\n",
            "temp27: 0.9511111111111111\n",
            "temp30: 0.999\n",
            "temp31: 0.988\n",
            "temp32: 0.9796747967479674\n",
            "temp33: 0.7145669291338582\n",
            "temp36: 0.9979591836734694\n",
            "temp35: 1.0\n",
            "temp37: 0.028\n",
            "temp38: 0.247\n",
            "temp39: 0.132\n",
            "temp40: 0.0016806722689075631\n",
            "temp41: 0.007407407407407408\n",
            "temp42: 0.07294117647058823\n",
            "temp43: 0.04\n",
            "temp44: 0.8914100486223663\n",
            "temp45: 1.0\n",
            "temp46: 1.0\n",
            "temp47: 0.995\n",
            "temp48: 0.998\n",
            "temp49: 1.0\n",
            "temp50: 0.398\n",
            "temp51: 0.03\n",
            "temp52: 0.201\n",
            "temp53: 0.0\n",
            "temp54: 0.04158790170132325\n",
            "temp58: 0.111\n",
            "temp59: 0.968\n",
            "temp60: 1.0\n",
            "temp61: 0.992\n",
            "temp63: 1.0\n",
            "temp62: 1.0\n",
            "temp68: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp9OnGFPjWK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hans = pd.read_csv('hans_predictions_with_labels.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt9idms8l1gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}