{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_model_on_hans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZVNpqEtmiK",
        "colab_type": "text"
      },
      "source": [
        "### In this notebook, we re-train BERT on the MNLI dataset plus examples from the HANS dataset. \n",
        "\n",
        "Before running the cells, change runtime to GPU. It is also required to upload the following:\n",
        "- ``` utils.py ``` a python script with a bunch of helper functions\n",
        "- ``` heuristics_train_set.txt ``` from the HANS dataset(https://github.com/tommccoy1/hans)\n",
        "- https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e, a zip file containing a script to download the MNLI dataset\n",
        "- ``` heuristics_evaluation_set.txt ``` from the HANS dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnESrpD4dJco",
        "colab_type": "code",
        "outputId": "61a24f26-1e6d-4155-f318-0dc9f3c42da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef7y1Vle2DjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llgUkEy2bK2j",
        "colab_type": "code",
        "outputId": "c1d7f888-77e1-4e26-a86e-0baa1da4ec7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\r\u001b[K     |▋                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 9.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 20.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=dd2497cdb91141e77bc24e132a09dc1b201ae405322f86b0f4a4dcd620daea0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIZYc76Pt7Nb",
        "colab_type": "code",
        "outputId": "0b8e3171-3368-47d3-e589-6ea8201e81ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install wget\n",
        "# unzipping glue datasets\n",
        "!unzip 60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Archive:  60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5.zip\n",
            "17b8dd0d724281ed7c3b2aeeda662b92809aadd5\n",
            "   creating: 60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5/\n",
            "  inflating: 60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGimkcB_t7rK",
        "colab_type": "code",
        "outputId": "de69bf0e-c862-4154-81ea-7f8aa20f1dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# downloading datasets\n",
        "!python '/content/60c2bdb54d156a41194446737ce03e2e-17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting SNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyEwNYFIvEn2",
        "colab_type": "text"
      },
      "source": [
        "-----------------\n",
        "### Reading datasets\n",
        "First, we read the MNLI dataset. This is done using the ``` read_data ``` function from the ``` utils.py ``` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW1FWGCBuLbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import read_data\n",
        "# reading in MNLI dataset\n",
        "train_premises, train_hypotheses, train_labels = read_data('/content/glue_data/MNLI/train.tsv')\n",
        "val_premises, val_hypotheses, val_labels = read_data('/content/glue_data/MNLI/dev_matched.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ7RNgJpxBvD",
        "colab_type": "text"
      },
      "source": [
        "For the next step, we must upload the \n",
        "``` heuristics_train_set.txt ``` \n",
        "file. This can be downloaded from https://github.com/tommccoy1/hans. We must also import the \n",
        "``` read_and_convert_hans_test ``` function from our ``` utils.py ``` file to read and convert the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78VazrR6uX5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import read_and_convert_hans\n",
        "hans_premises, hans_hypotheses, hans_pairIDs, hans_labels = read_and_convert_hans('/content/heuristics_train_set.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51xxbunA176K",
        "colab_type": "text"
      },
      "source": [
        "We now concatenate both datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOwjMs4BxfPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_premises = train_premises + hans_premises\n",
        "train_hypotheses = train_hypotheses + hans_hypotheses\n",
        "train_labels = train_labels + hans_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDGzcSBq43Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OPmyQnR_Cin",
        "colab_type": "code",
        "outputId": "ac284a53-2072-44d4-df58-ad6a65fb517b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_labels), len(train_premises)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(422703, 422703)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqksdnH2yY9",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the premises, hypotheses, and labels. This is done using helper functions from the ``` utils.py ``` file.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuT5Y8ev1vJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# loading bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YdtXh13KNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import tokenize_sentences\n",
        "train_inputs, train_ids, train_masks = tokenize_sentences(train_premises, train_hypotheses, 128, tokenizer)\n",
        "val_inputs, val_ids, val_masks = tokenize_sentences(val_premises, val_hypotheses, 128, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THHx9IQd4WWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import tokenize_labels_hans\n",
        "train_labels = tokenize_labels_hans(train_labels, tokenizer)\n",
        "val_labels = tokenize_labels_hans(val_labels, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMfwno135kkP",
        "colab_type": "text"
      },
      "source": [
        "Converting to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsyKLsAt3MoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13kn4-eG5n9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ids = torch.tensor(train_ids)\n",
        "val_ids = torch.tensor(val_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wWRyt-15rHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Iid_II5uWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsbYMaUy56f3",
        "colab_type": "text"
      },
      "source": [
        "Creating DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zY3ymUPgLGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xnXX8ie55xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create DataLoader for training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_ids, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "validation_data = TensorDataset(val_inputs, val_masks, val_ids, val_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-LnnN3U6i4x",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "### Using GPU for faster training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZkE_vbb6miU",
        "colab_type": "code",
        "outputId": "ccdbbc6f-c590-4bae-d0b8-c93ec4c1a92b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# check for GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# use GPU\n",
        "device = torch.device(\"cuda\")\n",
        "# confirm\n",
        "print('We are using a ', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-267a6d64190a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# use GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sioPbW-16an",
        "colab_type": "code",
        "outputId": "f814f630-ef93-4762-a26f-b9ed10082540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNG4Y0F6EYW",
        "colab_type": "text"
      },
      "source": [
        "------\n",
        "### Loading BERT\n",
        "We use 2 labels now instead of 3 since the HANS dataset uses ``` entailment ```and ```non-entailment``` instead of ``` entailment```, ```contradiction```, and ```neutral```. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IhXaiqv5_Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 2,  \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "# run model on GPU\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15NBGkk360ZG",
        "colab_type": "code",
        "outputId": "6a40ba08-bd96-4fe8-c047-0be485c2d3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 1 # manually train 3 times to avoid GPU connection issues\n",
        "\n",
        "total_steps = len(train_dataloader)*epochs\n",
        "\n",
        "# create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-507c285a7c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# create the learning rate scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m scheduler = get_linear_schedule_with_warmup(optimizer, \n\u001b[0m\u001b[1;32m     10\u001b[0m                                             \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Default value in run_glue.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                             num_training_steps = total_steps)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq0T1qME6_gs",
        "colab_type": "text"
      },
      "source": [
        "### Training BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_owe8Rj7All",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "seed = 72\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "  print('---------- Epoch %s ----------' % str(epoch))\n",
        "  # start clock\n",
        "  t0 = time.time()\n",
        "\n",
        "  # reset loss for epoch\n",
        "  total_loss = 0\n",
        "\n",
        "  # put model into training mode\n",
        "  model.train()\n",
        "\n",
        "  # for each batch of the training data\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "      time_elapsed = str(datetime.timedelta(seconds=int(round(time.time() - t0))))\n",
        "      print('\\t Batch %i of %i. Time elapsed: %s' % (step, len(train_dataloader), time_elapsed))\n",
        "    \n",
        "    # retrieve tensors from dataloader\n",
        "    # copy each to GPU using to(device)\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    sequence_ids = batch[2].to(device)\n",
        "    labels = batch[3].to(device)\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # perform forward pass\n",
        "    # the loss is returned\n",
        "    outputs = model(\n",
        "        input_ids = input_ids.long(),\n",
        "        attention_mask = attention_mask.long(),\n",
        "        token_type_ids = sequence_ids.long(),\n",
        "        labels = labels.long()\n",
        "        )\n",
        "    \n",
        "    loss = outputs[0]\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # perform backward pass to calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0 to help prevent \"exploding gradients\" \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "  try:\n",
        "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "    output_dir = '/content/saved_model'\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "    torch.save(model.state_dict(), output_dir)\n",
        "    torch.save(model, '/content/entire_model.pth')\n",
        "  except:\n",
        "    print('Saving Failed')\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "  loss_values.append(avg_train_loss)\n",
        "\n",
        "  print('--- Average Training Loss: %f' % avg_train_loss)\n",
        "\n",
        "  # Measure performance on validation set\n",
        "  t0 = time.time()\n",
        "  model.eval()\n",
        "\n",
        "  try:\n",
        "    hans_premises, hans_hypotheses, hans_pairIDs, hans_labels = read_and_convert_hans('/content/heuristics_evaluation_set.txt')\n",
        "    test_inputs, test_ids, test_masks = tokenize_sentences(hans_premises, hans_hypotheses, 128, tokenizer)\n",
        "    test_inputs = torch.tensor(test_inputs)\n",
        "    test_ids = torch.tensor(test_ids)\n",
        "    test_masks = torch.tensor(test_masks)\n",
        "    hans_pairIDs = torch.tensor(hans_pairIDs)\n",
        "    test_data = TensorDataset(test_inputs, test_masks, test_ids, hans_pairIDs)\n",
        "    test_sampler = RandomSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "    predictions = []\n",
        "    pair_ids = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "              \n",
        "        # Unpack the inputs from dataloader\n",
        "        input_ids, attention_mask, sequence_ids, batch_pair_ids = batch\n",
        "\n",
        "        # no need for grad since evaluation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "          outputs = model(input_ids = input_ids.long(),\n",
        "                              attention_mask = attention_mask.long(),\n",
        "                              token_type_ids = sequence_ids.long())\n",
        "              \n",
        "          logits = outputs[0]\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          batch_pair_ids = batch_pair_ids.to('cpu').numpy()\n",
        "\n",
        "          for i in range(0,len(logits)): \n",
        "            predictions.append(logits[i])\n",
        "            pair_ids.append('ex' + str(batch_pair_ids[i]))\n",
        "          \n",
        "    df = pd.DataFrame()\n",
        "    df['pairID'] = pair_ids\n",
        "    df['gold_label'] = predictions\n",
        "    df.to_csv('hans_predictions_post.csv', index=False)\n",
        "\n",
        "    print('---- HANS Testing Completed ----')\n",
        "  except:\n",
        "    print('---- HANS testing failed ----')\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "      # Unpack the inputs from dataloader\n",
        "      input_ids, attention_mask, sequence_ids, labels = batch\n",
        "\n",
        "      # no need for grad since evaluation\n",
        "      with torch.no_grad():        \n",
        "\n",
        "        outputs = model(input_ids = input_ids.long(),\n",
        "                          attention_mask = attention_mask.long(),\n",
        "                          token_type_ids = sequence_ids.long())\n",
        "          \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "          \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = np.sum(np.argmax(logits, axis=1).flatten() == label_ids.flatten())/len(label_ids)\n",
        "          \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "  try:\n",
        "    print(\"  Validation took: {:}\".format((datetime.timedelta(seconds=int(round(time.time() - t0)))))) \n",
        "  except:\n",
        "    continue\n",
        "\n",
        "print('Training complete.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdjF-hdXEovf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from utils import tokenize_sentences\n",
        "def read_and_convert_hans(filepath): \n",
        "  premises = []\n",
        "  hypotheses = []\n",
        "  pairIDs = []\n",
        "  gold_labels = []\n",
        "  first_line = True\n",
        "  with open(filepath) as file:\n",
        "    for fline in file:\n",
        "      line = re.split(r'\\t+', fline)\n",
        "      if first_line == True:\n",
        "        first_line = False\n",
        "        premises.append(line[5])\n",
        "        hypotheses.append(line[6])\n",
        "        continue\n",
        "      pairIDs.append(int(re.sub('ex', '', line[7])))\n",
        "      gold_labels.append(line[0])\n",
        "      premises.append(line[5])\n",
        "      hypotheses.append(line[6])\n",
        "    \n",
        "    #assert(len(pairIDs) == len(premises))\n",
        "    assert(len(premises) == len(hypotheses))\n",
        "    assert(len(pairIDs) == len(gold_labels))\n",
        "\n",
        "    return premises, hypotheses, pairIDs, gold_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR2xKjKbNxzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hans_premises, hans_hypotheses, hans_pairIDs, hans_labels = read_and_convert_hans('/content/heuristics_evaluation_set.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPcMhwl9NyUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs, test_ids, test_masks = tokenize_sentences(hans_premises, hans_hypotheses, 128, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BcV9p5wN1oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "hans_pairIDs = torch.tensor(hans_pairIDs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKEHtgzVN2Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = TensorDataset(test_inputs, test_masks, test_ids, hans_pairIDs)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwRCTucN4h1",
        "colab_type": "code",
        "outputId": "98ca68a5-f232-448a-a0f3-625e3f0724f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "predictions = []\n",
        "pair_ids = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "    # Unpack the inputs from dataloader\n",
        "    input_ids, attention_mask, sequence_ids, batch_pair_ids = batch\n",
        "\n",
        "    # no need for grad since evaluation\n",
        "    with torch.no_grad():        \n",
        "\n",
        "      outputs = model(input_ids = input_ids.long(),\n",
        "                          attention_mask = attention_mask.long(),\n",
        "                          token_type_ids = sequence_ids.long())\n",
        "          \n",
        "      logits = outputs[0]\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      batch_pair_ids = batch_pair_ids.to('cpu').numpy()\n",
        "\n",
        "      for i in range(0,len(logits)): \n",
        "        predictions.append(logits[i])\n",
        "        pair_ids.append('ex' + str(batch_pair_ids[i]))\n",
        "      \n",
        "df = pd.DataFrame()\n",
        "df['pairID'] = pair_ids\n",
        "df['gold_label'] = predictions\n",
        "df.to_csv('hans_predictions_post.csv', index=False)\n",
        "\n",
        "print('---- HANS Testing Completed ----')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2bf6503a25ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Tracking variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLT7I3xEN6zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output to CSV file to submit in kaggle competition\n",
        "df = pd.DataFrame()\n",
        "df['pairID'] = pair_ids\n",
        "df['gold_label'] = predictions\n",
        "df.to_csv('hans_predictions_post.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7oJ45eGODfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}